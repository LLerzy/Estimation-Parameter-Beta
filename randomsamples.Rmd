---
title: "RandomSample"
output: github_document
---

```{r setup, include=FALSE,echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
suppressPackageStartupMessages(source(file = "requiredfunctions.R"))
a1=2.2; b1=2.2; c1=2.2; d1=2.2
```

# Introduction

This Markdown presents a simulation framework for generating random samples from a newly proposed bivariate probability distribution, characterized by the parameter vector $\phi = (a, b, c, d)$ and the following joint probability density function:

$$
f_{\phi}(y_1,y_2)= \dfrac{1}{beta(a,b)beta(c,d)}\ y_1^{a-1}y_2^{b-1}(y_1+y_2)^{d-(a+b)} (y_1+y_2+1)^{-c-d},\hspace{1cm}y_1,y_2\in\mathbb{R}^+,
$$

where $\mathrm{beta}(\cdot, \cdot)$ denotes the Beta function. Additionally, the joint moment of order $l = l_1 + l_2$ for this distribution is given by:

$$
E_{\phi}[Y_1^{l_1} Y_2^{l_2}] \propto beta(c-l,l+d)\times beta(l_1+a,l_2+b)
$$

provided that $c > 2$, ensuring the existence of the moment.

The simulation approach integrates two key Markov Chain Monte Carlo (MCMC) techniques. First, the **Metropolis-Hastings algorithm with an adaptive random walk** is employed to generate samples from the full conditional distribution of $X_1$ given $X_2$. Second, **Gibbs Sampling** is applied to produce samples from the joint distribution of the vector $(X_1, X_2)$. Finally, these samples are transformed into realizations of the target random vector $(Y_1, Y_2)$ through the following bijective transformation:

$$
(Y_1,Y_2)=\left(X_1\left(\dfrac{X_1(1-X_1)}{X_2}-1\right),(1-X_1)\left(\dfrac{X_1(1-X_1)}{X_2}-1\right)\right).
$$

This method ensures flexible and efficient sampling from the proposed bivariate distribution, enabling further analysis and characterization of its properties.

# Generating Samples of $X_1$ Given $X_2$

## Conditional Distribution of $X_1$ Given $X_2$

The non-normalized conditional density function of $X_1$ given $X_2 = v$ is expressed as:

$$
f_{\phi}(x_1|x_2)\propto x_1^{a-c-d}(x_1(1-x_1)-x_2)^{d-1}(1-x_1)^{b-c-d}
$$

where $\phi = (a, b, c, d)$ is the parameter vector of the distribution. To illustrate the behavior of this conditional density, we set the hyperparameters to $\phi = (a1, b1, c1, d1) = (2.2, 2.2, 2.2, 2.2)$ and plot the corresponding density curves for three different values of the conditioning variable $X_2$.

```{r, echo=TRUE, warning=FALSE,fig.width=7, fig.height=4}
a1=2.2; b1=2.2; c1=2.2; d1=2.2
Graph_Fc_X1(v1 = 0.05,"v=0.05",v2 = 0.10,"v=0.10",v3 = 0.20,"v=0.20",ae = a1,be = b1,ce = c1,de = d1)
```

It is observed that the conditional density is symmetric for the configuration of $\phi$ values and that it decreases for $X_2$ values close to 0.25.

## Metropolis-Hastings Algorithm with Adaptive Random Walk

Using the Metropolis-Hastings Algorithm with adaptive random walk (MHARW), a chain of size $N=10^5$ is generated with a precision of 3 in the proposal distribution, a thinning of 25, and a burn-in of 5000. The graph of the resulting density shows a similar behavior to the one obtained previously.

```{r echo=TRUE, warning=FALSE, results='hide',fig.width=7, fig.height=5}
test=Gen_FC_X1_X2(N=10^5, prop_prec=3, a=a1, b=b1, c=c1, d=d1, v=0.05, option = "all", thin=25, burnin=5000,target_acceptance = 0.4)
plot(density(test$thinned_chain),main="Non-normalized Conditional Distribution Density of X_1 Given X_2",xlab=expression("Values of " ~ X[1]~"given"~X[2]==0.05))
```

## Convergence Monitoring of the Simulation Method

To assess the convergence of the Markov chain generated by the proposed algorithm, key diagnostics are monitored: the **acceptance rate**, **effective sample size (ESS)**, and the **R-hat diagnostic** (Gelman-Rubin statistic). These metrics are evaluated across different values of the **precision parameter** (used in the proposal distribution) and the **conditioning variable** $X_2 = v$

```{r warning=FALSE, echo=TRUE, results='hide'}
df_ess_ar <- results_Mon_Measure(N=10^4, prop_prec_values=seq(1, 20, by = 1), a=a1, b=b1, c=c1, d=d1, v_values=seq(0.01, 0.24, length.out = 10),thin = 2,burnin = 1000,target_acceptance = 0.4)
 df_rhat <- results_Mon_R_Hat(N=10^4, prop_prec_values=seq(1, 20, by = 1), a=a1, b=b1, c=c1, d=d1, v_values=seq(0.01, 0.24, length.out = 10),thin = 2,burnin = 1000,target_acceptance = 0.4)
```

The desired convergence behavior is as follows: - The **acceptance rate** (Figure 3b) should ideally remain between **0.3 and 0.7**. - The **effective sample size** (Figure 3a) should be close to the post burn-in sample size. For the configuration used in this study, the ESS is expected to exceed **465** and approach $(10^4 - 1000) / 2 = 4500$, given the thinning interval of 2 and burn-in of 1000 from a total of 10,000 iterations.

Additionally, the **R-hat diagnostic** (Figure 3c) is computed to evaluate convergence across multiple chains. This statistic compares the variance between chains to the variance within chains. An **R-hat value close to 1** indicates that the chains have converged to the same target distribution, suggesting that the generated samples are representative of the underlying distribution. Conversely, **R-hat values significantly greater than 1** suggest a lack of convergence, and it is recommended to extend the simulation to improve convergence and ensure more reliable estimates.

The panel of convergence plots (Figures 3a–3c) provides a comprehensive view for selecting appropriate values of the **precision parameter** based on the **conditioning variable** $X_2 = v$, ensuring both the efficiency and stability of the sampling process.

```{r fig.width=8, fig.height=10, warning=FALSE, echo=TRUE}
# Example usage:
Generate_Figure3_Panels(df_ess_ar, df_rhat, prop_prec_values=seq(1, 20, by = 1))
```

## Another Random Sample from the Conditional Distribution of $X_1$ given $X_2$

A new sample of size $N = 10^5$ was generated from the conditional distribution of $X_1$ given $X_2 = v = 0.05$, using the parameter vector $\phi = (2.2, 2.2, 2.2, 2.2)$. The simulation was configured with a precision parameter of 3, a thinning interval of 25, a burn-in of 5000 iterations, and a target acceptance rate of 0.4. This acceptance rate governs the adaptive adjustment of the precision parameter $\varphi$ during the burn-in period, allowing the algorithm to automatically increase or decrease precision in response to the observed acceptance rate. This mechanism facilitates faster reduction in autocorrelation and improves the overall mixing of the chain.

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=8, fig.height=5.5}
ExampleFC_X1_X2=Gen_FC_X1_X2(N=10^5, prop_prec = 3, a=a1, b=b1, c=c1, d=d1, v=0.05, option="all",thin = 25, burnin = 5000, X10_given = "random", target_acceptance = 0.4, dig_tol=15)
Graphs(as.data.frame(ExampleFC_X1_X2$thinned_chain), "X1", width = 40, lscatt = 0.3, uscatt = 0.3)
```

The results of the `ExampleFC_X1_X2` simulation are summarized in the following table, which reports the acceptance rate before and after the burn-in period, the effective sample size (ESS), the final chain length, and the final value of the adjusted precision parameter. The adjustment process is limited to the burn-in phase of the simulation.

```{r}
library(knitr)
library(kableExtra)

df <- data.frame(
  "Acceptance Rate" = ExampleFC_X1_X2$acc_rate,
  "Acceptance Rate Post Burn-in" = ExampleFC_X1_X2$acc_rate_pos_burnin,
  "ESS" = effectiveSize(ExampleFC_X1_X2$thinned_chain),
  "Length" = length(ExampleFC_X1_X2$thinned_chain),
  "Precision" = ExampleFC_X1_X2$precision,
  row.names = "Chain Measurements"
)
df
```

## Convergence Monitoring with Fixed Seeds

In the previous simulations, the random number generator seed was selected randomly for each run. In this section, we instead fix nine user-defined seeds to examine the impact of initial conditions on the convergence behavior of the Markov chains. Specifically, we assess the stability of the posterior mean estimate for each generated chain.

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=8.5, fig.height=6}
N=10^5;burnin=5000;thin=25; target_acceptance=0.4
seedgiv=seq(0.1,0.9,0.1)
ExampleFC_X1_X2_seedgiven=matrix(data=NA,nrow = length(seq((burnin+1), N, by = thin)),ncol=9,dimnames=list(list(),list("0.1","0.2","0.3","0.4","0.5","0.6","0.7","0.8","0.9")))
for (i in 1:9) {
  ExampleFC_X1_X2_seedgiven[,i]=Gen_FC_X1_X2(N=N, prop_prec = 3, a=a1, b=b1, c=c1, d=d1, v=0.05, option="all", thin = thin, burnin = burnin, X10_given = seedgiv[i], dig_tol = 10,target_acceptance)$thinned_chain
}
ExampleFC_X1_X2_seedgiven=as.data.frame(ExampleFC_X1_X2_seedgiven)

# Matrix of cumsum of all simulations obtained.
N=nrow(ExampleFC_X1_X2_seedgiven)
ExampleFC_X1_X2_seedgiven_cum=apply(ExampleFC_X1_X2_seedgiven,2,cumsum)/(1:N)
ExampleFC_X1_X2_seedgiven_cum=as.data.frame(ExampleFC_X1_X2_seedgiven_cum)

# Matrix organization for plotting.
ExampleFC_X1_X2_seedgiven_cum=gather(ExampleFC_X1_X2_seedgiven_cum,key="Seed",value="FCmu")
ExampleFC_X1_X2_seedgiven_cum$Sim=rep(1:N,9)
# Function to apply a consistent theme
  custom_theme <- theme_minimal(base_size = 10) +
    theme(
      axis.title = element_text(size = 12),
      axis.text = element_text(size = 9),
      legend.title = element_text(size = 10),
      legend.text = element_text(size = 9),
      plot.margin = margin(10, 10, 10, 10)
    )
# Graphic
Piece1_Grap_FC=ggplot(ExampleFC_X1_X2_seedgiven_cum,aes(x=Sim ,y=FCmu,group= Seed, color=Seed))+geom_line()+ylab(expression("Accumulated Average " ~ hat(X)[1]^(t)))+xlab("Chain Iterations (t)")+xlim(0,50)+labs(title = "(a)")+custom_theme

Piece2_Grap_FC=ggplot(ExampleFC_X1_X2_seedgiven_cum,aes(x=Sim ,y=FCmu,group= Seed, color=Seed))+geom_line()+ylab(expression("Accumulated Average " ~ hat(X)[1]^(t)))+xlab("Chain Iterations (t)")+xlim(0,250)+labs(title = "(b)")+custom_theme

Full_Grap_FC=ggplot(ExampleFC_X1_X2_seedgiven_cum,aes(x=Sim ,y=FCmu,group= Seed, color=Seed))+geom_line()+ylab(expression("Accumulated Average " ~ hat(X)[1]^(t)))+xlab("Chain Iterations (t)")+labs(title = "(c)")+custom_theme

grid.arrange(arrangeGrob(Piece1_Grap_FC, Piece2_Grap_FC, Full_Grap_FC,ncol=2, nrow=2, widths=c(2, 2), heights=c(2,2),layout_matrix=rbind(c(1,2),c(3,3))),bottom="Convergence control using averaging")
```

The posterior mean of each \$X_1\$ chain generated under the nine fixed seeds is presented in the following table:

```{r}
round(apply(ExampleFC_X1_X2_seedgiven[1:3800,], 2, mean),3)
```

# Gibbs Sampling

## Random Samples for the Vector $(X_1, X_2)$

### Convergence Monitoring

The Gibbs Sampling algorithm is used to generate a random sample of the vector $(X_1, X_2)$. For the convergence monitoring of the chains generated for $X_1$ and $X_2$, figures are presented that include density versus histogram, trace, cumulative mean, and autocorrelation.

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=8, fig.height=5.5}
N=10^5
Example_Joint_Dist=Gen_Joint_Dist(N1 = N,N2 = 2,prop_prec = 3,a1,b1,c1,d1,thin = 2,X10_given ="random",target_acceptance = 0.4,batch_adapt_acceptance_rate=100)
burnin=5000;thin=25
Graphs(as.data.frame(Example_Joint_Dist$X2[seq((burnin+1), N, by=thin)]),"X2",width = 40,uscatt = 0.06,lscatt = 0.05)
```

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=8, fig.height=5.5}
Graphs(as.data.frame(Example_Joint_Dist$X1[seq((burnin+1), N, by=thin)]),"X1",width = 40,uscatt = 0.25,lscatt = 0.2)
```

### Contour Plots and Scatter Plot for the Generated Sample of $(X_1, X_2)$

The scatter plot of the generated sample `Example_Joint_Dist` is plotted over the contour lines associated with the bivariate distribution $(X_1, X_2)$ with parameters $\phi = (2.2, 2.2, 2.2, 2.2)$.

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=7, fig.height=4}
mu1=seq(0,1,length=10^3)
s1=seq(0,0.25,length=10^3)
dta=expand.grid(X1 = mu1, X2 = s1) %>% 
  mutate(Z = ifelse(X1 * (1 - X1) > X2, X2^(c1 - 1) * (X1 * (1 - X1) - X2)^(d1 - 1) / (X1^(c1 + d1 - a1) * (1 - X1)^(c1 + d1 - b1)), NA))
ggplot()+geom_contour(aes(dta$X1, dta$X2, z = (dta$Z)),bins = 50) + 
  geom_point(aes(x=Example_Joint_Dist$X1[seq((burnin+1), N, by=thin)], y=Example_Joint_Dist$X2[seq((burnin+1), N, by=thin)]), colour="red",size=0.2) + 
  xlab(expression(X[1])) + ylab(expression(X[2])) + 
  labs(title = "Contour Plots and Scatter Plot of a Random Sample")+custom_theme
```

## Random Samples for the Vector $(Y_1, Y_2)$

### Convergence Monitoring

The generated sample `Example_Joint_Dist` for the vector $(X_1, X_2)$ is now transformed into a sample for the vector $(Y_1, Y_2)$ using the transformation presented at the beginning of this document. For convergence monitoring of the chains for $Y_1$ and $Y_2$, the density and histogram, trace plot, cumulative mean, and autocorrelation are used.

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=8, fig.height=5.5}
piece=(Example_Joint_Dist$X1[seq((burnin+1), N, by=thin)]*(1-Example_Joint_Dist$X1[seq((burnin+1), N, by=thin)])/ (Example_Joint_Dist$X2[seq((burnin+1), N, by=thin)])-1)
alpha=Example_Joint_Dist$X1[seq((burnin+1), N, by=thin)]*piece
beta=(1-Example_Joint_Dist$X1[seq((burnin+1), N, by=thin)])*piece
Graphs(as.data.frame(alpha),"Y1",width = 40,uscatt = 10,lscatt = 10)
```

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=8, fig.height=5.5}
Graphs(as.data.frame(beta),"Y2",width = 40,uscatt = 10,lscatt = 10)
```

### Contour Plots and Scatter Plot for the Generated Sample of $(Y_1, Y_2)$

The scatter plot of the generated sample for the random vector $(Y_1, Y_2)$ is plotted over the contour lines associated with the bivariate distribution $(Y_1, Y_2)$ with parameters $\phi = (2.2, 2.2, 2.2, 2.2)$.

```{r, echo=TRUE, warning=FALSE, results='hide', fig.width=7, fig.height=4}
alp1=seq(0,18,length=10^3)
bet1=seq(0,10,length=10^3)
dta=expand.grid(X1 = alp1, X2 = bet1) %>%
  mutate(Z = X1^(a1-1)*X2^(b1-1)*(X1+X2)^(d1-a1-b1)*(X1+X2+1)^(-c1-d1)/(beta(a1,b1)*beta(c1,d1)))
ggplot()+geom_contour(aes(dta$X1, dta$X2, z = dta$Z),bins = 50)+
  geom_point(aes(x=alpha,y=beta),colour="red",size=0.3)+xlab(expression(Y[1])) + 
  ylab(expression(Y[2])) + 
  labs(title = "Contour Plots and Scatter Plot of a Random Sample")+xlim(c(0,5)) + 
  ylim(c(0,2))+custom_theme
```

# Comparison Between Numerical and Analytical Moments

The function `Measure_Diagnostic` was applied to the sample `Example_Joint_Dist` to compute the numerical moments, the corresponding theoretical values, and their differences for the vector $(Y_1, Y_2)$. The function was executed using a burn-in of 5000 iterations and a thinning interval of 25.

```{r, echo=TRUE, warning=FALSE, results='hide'}
results_measure_diag=Measure_Diagnostic(data1 = Example_Joint_Dist$X1,data2 = Example_Joint_Dist$X2, var ="transform", digits = 4, a = a1, b = b1, c = c1, d = d1, burnin = 5000, thin = 25)
```

The table below summarizes the numerical estimates, theoretical values, and their respective differences for key descriptive measures.

```{r}
library(knitr)
library(kableExtra)

numerical_df <- as.data.frame.matrix(t(results_measure_diag$Numerical))
analytical_df <- as.data.frame.matrix(t(results_measure_diag$Analytical))
differences_df <- as.data.frame.matrix(t(results_measure_diag$Differences)) 

# Combine the three data frames into one
combined_df <- cbind(numerical_df, analytical_df[,1],differences_df[,1])
names(combined_df)=c("Numerical Results", "Theoretical Results", "Differences")

# The table is created with kable
combined_df %>%
  kable(caption = "Summary Statistics for Different Measures", digits = 3)
```

The Gelman–Rubin diagnostic (R-hat) was computed for the chains of $Y_1$ and $Y_2$ generated using the same configuration: $10^5$ iterations, a burn-in of 5000, thinning of 25, and a precision parameter $\varphi = 3$. The R-hat calculation was based on three independent chains, as illustrated in the following code block.

```{r}
sample1 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a1,b1,c1,d1,thin = 2,X10_given ="random",target_acceptance = 0.4)

sample2 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a1,b1,c1,d1,thin = 2,X10_given ="random",target_acceptance = 0.4)

sample3 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a1,b1,c1,d1,thin = 2,X10_given ="random",target_acceptance = 0.4)

burnin=5000;thin=25

Gelm_Rud_X1 = gelman.diag(list(mcmc(sample1$X1[seq((burnin+1), N, by=thin)]), mcmc(sample2$X1[seq((burnin+1), N, by=thin)]),mcmc(sample3$X1[seq((burnin+1), N, by=thin)])))$psrf[1]

Gelm_Rud_X2 = gelman.diag(list(mcmc(sample1$X2[seq((burnin+1), N, by=thin)]), mcmc(sample2$X2[seq((burnin+1), N, by=thin)]),mcmc(sample3$X2[seq((burnin+1), N, by=thin)])))$psrf[1]

print(c(Gelm_Rud_X1,Gelm_Rud_X2))
```

# Comparison Using an Alternative Parameter Configuration

To further evaluate the performance of the simulation scheme, an alternative parameter vector $\phi = (3, 6, 3, 6)$ was considered. The simulation setup remains the same as in the previous sections, except that the thinning factor was reduced to 5.

```{r, echo=TRUE, warning=FALSE, results='hide'}
a2=3;b2=6;c2=3;d2=6
Example_Joint_Dist1=Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec=3,a = a2,b = b2,c = c2,d = d2,thin = 2, X10_given = "random",target_acceptance = 0.4)

results_measure_diag1=Measure_Diagnostic(data1 = Example_Joint_Dist1$X1,data2 = Example_Joint_Dist1$X2, var ="transform", digits = 3, a = a2, b = b2, c = c2, d = d2, burnin = 5000, thin = 25)
```

The table below presents the updated numerical estimates, theoretical values, and their differences under this new configuration.

```{r}
library(knitr)
library(kableExtra)

numerical_df <- as.data.frame.matrix(t(results_measure_diag1$Numerical))
analytical_df <- as.data.frame.matrix(t(results_measure_diag1$Analytical))
differences_df <- as.data.frame.matrix(t(results_measure_diag1$Differences)) 

# Combine the three data frames into one
combined_df <- cbind(numerical_df, analytical_df[,1],differences_df[,1])
names(combined_df)=c("Numerical Results", "Theoretical Results", "Differences")

# The table is created with kable
combined_df %>%
  kable(caption = "Summary Statistics for Different Measures", digits = 3) 
```

The R-hat values for the chains of $Y_1$ and $Y_2$, generated under the new parameter configuration, burn-in, and thinning settings, are reported below:

```{r}
sample1 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a2,b2,c2,d2,thin = 2,X10_given ="random",target_acceptance = 0.4)

sample2 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a2,b2,c2,d2,thin = 2,X10_given ="random",target_acceptance = 0.4)

sample3 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a2,b2,c2,d2,thin = 2,X10_given ="random",target_acceptance = 0.4)

burnin=5000;thin=25

Gelm_Rud_X1 = gelman.diag(list(mcmc(sample1$X1[seq((burnin+1), N, by=thin)]), mcmc(sample2$X1[seq((burnin+1), N, by=thin)]),mcmc(sample3$X1[seq((burnin+1), N, by=thin)])))$psrf[1]

Gelm_Rud_X2 = gelman.diag(list(mcmc(sample1$X2[seq((burnin+1), N, by=thin)]), mcmc(sample2$X2[seq((burnin+1), N, by=thin)]),mcmc(sample3$X2[seq((burnin+1), N, by=thin)])))$psrf[1]

print(c(Gelm_Rud_X1,Gelm_Rud_X2))
```

# Application

```{r, echo=TRUE, warning=FALSE, results='hide'}
N=10^5
a3=141.6457; b3=2263.205; c3=39.48316; d3=3092.636
Example_Joint_Dist3=Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec=3,a = a3,b = b3,c = c3,d = d3,thin = 2, X10_given = "random",target_acceptance = 0.4)

results_measure_diag3=Measure_Diagnostic(data1 = Example_Joint_Dist3$X1,data2 = Example_Joint_Dist3$X2, var ="transform", digits = 5, a = a3, b = b3, c = c3, d = d3, burnin = 5000, thin = 5)
```

```{r}
library(knitr)
library(kableExtra)

numerical_df <- as.data.frame.matrix(t(results_measure_diag3$Numerical))
analytical_df <- as.data.frame.matrix(t(results_measure_diag3$Analytical))
differences_df <- as.data.frame.matrix(t(results_measure_diag3$Differences)) 

# Combine the three data frames into one
combined_df <- cbind(numerical_df, analytical_df[,1],differences_df[,1])
names(combined_df)=c("Numerical Results", "Theoretical Results", "Differences")

# The table is created with kable 
combined_df %>%
  kable(caption = "Summary Statistics for Different Measures", digits = 3) 
```

```{r}
sample1 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a3,b3,c3,d3,thin = 2,X10_given ="random",target_acceptance = 0.4)

sample2 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a3,b3,c3,d3,thin = 2,X10_given ="random",target_acceptance = 0.4)

sample3 = Gen_Joint_Dist(N1 = 10^5,N2 = 2,prop_prec = 3,a3,b3,c3,d3,thin = 2,X10_given ="random",target_acceptance = 0.4)

burnin=5000;thin=25

Gelm_Rud_X1 = gelman.diag(list(mcmc(sample1$X1[seq((burnin+1), N, by=thin)]), mcmc(sample2$X1[seq((burnin+1), N, by=thin)]),mcmc(sample3$X1[seq((burnin+1), N, by=thin)])))$psrf[1]

Gelm_Rud_X2 = gelman.diag(list(mcmc(sample1$X2[seq((burnin+1), N, by=thin)]), mcmc(sample2$X2[seq((burnin+1), N, by=thin)]),mcmc(sample3$X2[seq((burnin+1), N, by=thin)])))$psrf[1]

print(c(Gelm_Rud_X1,Gelm_Rud_X2))
```
